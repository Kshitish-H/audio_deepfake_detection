{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63fe6db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5aa3783",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASVspoofDataset(Dataset):\n",
    "    def __init__(self, data_dir, protocol_file, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "\n",
    "        with open(protocol_file, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                filename = parts[1]\n",
    "                label = 1 if parts[-1] == 'spoof' else 0\n",
    "                self.data.append((filename, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename, label = self.data[idx]\n",
    "        file_path = os.path.join(self.data_dir, filename + '.flac')\n",
    "\n",
    "        try:\n",
    "            waveform, sr = torchaudio.load(file_path)\n",
    "        except:\n",
    "            # Corrupt or unreadable file, return dummy tensor\n",
    "            return torch.zeros(1, 128, 128), label\n",
    "\n",
    "        if self.transform:\n",
    "            features = self.transform(waveform)\n",
    "        else:\n",
    "            features = waveform\n",
    "\n",
    "        return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e959d63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kshitish\\anaconda3\\Lib\\site-packages\\torchaudio\\functional\\functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mel_transform = nn.Sequential(\n",
    "    MelSpectrogram(sample_rate=16000, n_fft=400, hop_length=160, n_mels=128),\n",
    "    AmplitudeToDB()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fce9cf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    specs, labels = zip(*batch)\n",
    "    # Resize or pad all to 128x128\n",
    "    processed = []\n",
    "    for spec in specs:\n",
    "        if spec.shape[-1] < 128:\n",
    "            pad_amt = 128 - spec.shape[-1]\n",
    "            spec = F.pad(spec, (0, pad_amt))\n",
    "        elif spec.shape[-1] > 128:\n",
    "            spec = spec[:, :, :128]\n",
    "        processed.append(spec)\n",
    "    specs_tensor = torch.stack(processed)\n",
    "    labels_tensor = torch.tensor(labels)\n",
    "    return specs_tensor, labels_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ea24065",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAttentionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.attn = nn.Sequential(\n",
    "            nn.Conv2d(64, 32, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 1, 1),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64 * 64 * 64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))  # (B, 32, 128, 128)\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # (B, 64, 64, 64)\n",
    "\n",
    "        attn_weights = self.attn(x)  # (B, 1, 64, 64)\n",
    "        x = x * attn_weights  # Apply attention\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60c36b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs=10, lr=0.001):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss, correct = 0.0, 0\n",
    "        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "        acc = correct / len(train_loader.dataset)\n",
    "        print(f\"Train Loss: {total_loss:.4f}, Accuracy: {acc:.4f}\")\n",
    "\n",
    "        evaluate_model(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd3b074e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, val_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "    acc = correct / len(val_loader.dataset)\n",
    "    print(f\"Validation Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1be31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "data_root = \"./PA/ASVspoof2019_PA_train/flac\"\n",
    "protocol_train = \"./PA/ASVspoof2019_PA_cm_protocols/ASVspoof2019.PA.cm.train.trn.txt\"\n",
    "protocol_dev = \"./PA/ASVspoof2019_PA_cm_protocols/ASVspoof2019.PA.cm.dev.trl.txt\"\n",
    "data_dev_root = \"./PA/ASVspoof2019_PA_dev/flac\"\n",
    "\n",
    "train_dataset = ASVspoofDataset(data_root, protocol_train, transform=mel_transform)\n",
    "dev_dataset = ASVspoofDataset(data_dev_root, protocol_dev, transform=mel_transform)\n",
    "\n",
    "\n",
    "#i am using a subset because of resource constraints, if you wish to train on the entire dataset, remove the train_subset and dev_subset, and uncomment the train_loader and val_loader that are commented, remove the other ones.\n",
    "train_subset = Subset(train_dataset, indices=range(5000))\n",
    "dev_subset = Subset(dev_dataset, indices=range(1000))\n",
    "\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "# val_loader = DataLoader(dev_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(dev_subset, batch_size=16, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "909a88dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1902e0347d1461ca71c6d7d0ca62e54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0432, Accuracy: 0.9994\n",
      "Validation Accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b740109c05b4e34aa33c07ed19f3621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6b70ed880c7410fa676c43e0e316d48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0000, Accuracy: 1.0000\n",
      "Validation Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model = ConvAttentionModel()\n",
    "train_model(model, train_loader, val_loader, epochs=3, lr=0.001)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
